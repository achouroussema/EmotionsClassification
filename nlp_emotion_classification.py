# -*- coding: utf-8 -*-
"""NLP Emotion Classification.ipynb

Automatically generated by Colaboratory.


!pip install --upgrade tensorflow keras --quiet

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px

import os


import re, string
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import nltk, subprocess

import warnings
warnings.filterwarnings(action="ignore", category=FutureWarning)

from google.colab import files
import zipfile
import io
import pandas as pd

# Upload the zip file
uploaded = files.upload()

# Specify the file name of the uploaded zip file
zip_filename = next(iter(uploaded))

# Unzip the uploaded file
zip_file = zipfile.ZipFile(io.BytesIO(uploaded[zip_filename]), 'r')
zip_file.extractall()

# Specify the name of the CSV file in the zip file
csv_filename = 'emotions.csv'

# Load the CSV file into a DataFrame
df = pd.read_csv(csv_filename)

# Display the DataFrame
print(df.head())

df.shape

# the number of missing values in each column of the DataFrame
df.isnull().sum()

# remove duplicate values
df = df.drop_duplicates()

sentences = df['text']
labels = df['label']

#remove special character
import nltk
nltk.download('wordnet')

lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words("english"))

def transform_data(review):
    review = review.lower()
    review = re.sub("^a-zA-Z0-9", ' ', review)
    review = re.sub('<.*?>', ' ', review)
    review = "".join([x for x in review if x not in string.punctuation])
    review = review.split()
    review = [lemmatizer.lemmatize(x) for x in review if x not in stop_words]
    review = " ".join(review)
    return review

sentences = sentences.apply(transform_data)

word_counts= []
word_count = [word_counts.append(len(sentence.split(" "))) for sentence in sentences]

max(word_counts)

from keras.preprocessing.text import one_hot
from keras.preprocessing.sequence import pad_sequences

vocab_size = 10000

# one hot representation
onehot_rep = [one_hot(word, vocab_size) for word in sentences]

print(sentences[0])
print(onehot_rep[0])

max_len = 50
embedding_docs = pad_sequences(
    sequences=onehot_rep,
    padding='post',
    maxlen=max_len
)
embedding_docs.shape

print(sentences[0])
print(onehot_rep[0])
print(embedding_docs[0])

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    embedding_docs,
    labels,
    train_size=0.8,
    random_state=42
)

print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)

training_labels_final = np.array(X_train)
testing_labels_final = np.array(y_train)

from keras.models import Sequential
from keras.layers import Embedding, Bidirectional, Dense, LSTM, SpatialDropout1D

embedding_dim = 128

model = Sequential()
model.add(Embedding(vocab_size, embedding_dim, input_length=embedding_docs.shape[1]))
model.add(Bidirectional(LSTM(embedding_dim,return_sequences=True)))
model.add(Bidirectional(LSTM(embedding_dim)))
model.add(Dense(6, activation='relu'))
model.add(Dense(6, activation='softmax'))

model.summary()

#Training
from keras.optimizers import Adam

model.compile(
    loss = 'sparse_categorical_crossentropy',
    optimizer = Adam(),
    metrics=['accuracy']
)

from keras.callbacks import EarlyStopping

history = model.fit(
    X_train, y_train,
    epochs=12,
    batch_size=512,
    validation_split=0.2,
    callbacks=[EarlyStopping(monitor='val_loss',patience=7, min_delta=0.0001)]
)

def plot_graphs(history, string):
  plt.plot(history.history[string])
  plt.plot(history.history['val_'+string])
  plt.xlabel("Epochs")
  plt.ylabel(string)
  plt.legend([string, 'val_'+string])
  plt.show()

plot_graphs(history, "accuracy")
plot_graphs(history, "loss")

def predict_emotions(model, new_sentences, maxlen=max_len, show_padded_sequence=True ):
    new_sequences = []

    for word in sentences:
        new_sequences.append(one_hot(word, 1000))

    trunc_type='pre'
    padding_type='pre'

    # Pad all sequences for the new reviews
    new_emotion_padded = pad_sequences(new_sequences, maxlen=max_len,
                                 padding=padding_type, truncating=trunc_type)

    classes = model.predict([new_emotion_padded])

  # The closer the class is to 1, the more positive the review is
    for x in range(len(new_sentences)):
        if (show_padded_sequence):
            print(new_emotion_padded[x])

        print(new_sentences[x])
        print(classes[x])
        print("\n")

# Use the model to predict some reviews
emotions = ["I just feel really helpless and heavy hearted"]

predict_emotions(model, emotions)
